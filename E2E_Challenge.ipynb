{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWcKKTgEaUB7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import csv\n",
        "import random\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTLKD23gafZt",
        "outputId": "ff66080e-7eff-4061-ebed-b94f83d0824b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GXlsrBZZtQ61",
        "outputId": "e12e3ebb-c368-4dc5-aa7f-a1f3ea59e3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Extraction and Preprocessing"
      ],
      "metadata": {
        "id": "cWhM6y_3hcUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_examples(filename):\n",
        "    with open(filename) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
        "        next(csv_reader)\n",
        "        example_list = [(mr, ref) for (mr, ref) in csv_reader]\n",
        "    return example_list"
      ],
      "metadata": {
        "id": "wVyM8bBUai3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 50\n",
        "START = '<s>'\n",
        "END = '</s>'\n",
        "UNK = '<unk>'\n",
        "PAD = '<pad>'\n",
        "def extract_mr_ref(example_list):\n",
        "    example_list_ = []\n",
        "    ref_tokens_list = []\n",
        "    mr_tokens_list = []\n",
        "    lengths_list = []\n",
        "    for mr, ref in example_list:\n",
        "        # Reference Tokens\n",
        "        ref_tokens = nltk.word_tokenize(ref.lower(), language='english')\n",
        "        if len(ref_tokens) > MAX_LEN:\n",
        "            continue\n",
        "        lengths_list.append(len(ref_tokens))\n",
        "        if len(ref_tokens) < MAX_LEN:\n",
        "            ref_tokens += [PAD]*(MAX_LEN-len(ref_tokens))\n",
        "        ref_tokens = [START,] + ref_tokens + [END,]\n",
        "\n",
        "        # Meaning Representation Tokens\n",
        "        mr_tokens = mr.lower().split(',')\n",
        "        mr_tokens = [token.strip() for token in mr_tokens]\n",
        "\n",
        "        example_list_.append((mr, ref))\n",
        "        ref_tokens_list.append(ref_tokens)\n",
        "        mr_tokens_list.append(mr_tokens)\n",
        "    return example_list_, ref_tokens_list, mr_tokens_list, lengths_list"
      ],
      "metadata": {
        "id": "8lxD_km9bEiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting our examples"
      ],
      "metadata": {
        "id": "tCNo_JvohoGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = get_examples('train.txt')\n",
        "train_examples, train_ref_tokens, train_mr_tokens, train_lengths = extract_mr_ref(train_examples)"
      ],
      "metadata": {
        "id": "hzD9lbf1kBeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building our vocabulary"
      ],
      "metadata": {
        "id": "VDrBa9X0hqyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now build vocabulary\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "ref_vocab = build_vocab_from_iterator(train_ref_tokens, min_freq=1,  ## TODO: Maybe this will cause issues\n",
        "                                      max_tokens=50000, specials=[UNK,])\n",
        "ref_vocab.set_default_index(ref_vocab[UNK])\n",
        "mr_vocab = build_vocab_from_iterator(train_mr_tokens, min_freq=1,\n",
        "                                     max_tokens=50000)\n",
        "train_ref_tokens = [torch.Tensor([ref_vocab[token] for token in sentence]).type(torch.long).to(device) for sentence in train_ref_tokens]\n",
        "train_mr_tokens = [torch.Tensor([mr_vocab[token] for token in sentence]).type(torch.long).to(device) for sentence in train_mr_tokens]"
      ],
      "metadata": {
        "id": "JFRysYGGmQ8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1dcc22c-a8d7-414e-971b-fccfed743f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "2rCKwwq-ht_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "DAhCB_dyhyPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=hidden_size)\n",
        "        self.recurrent = nn.GRU(input_size=hidden_size,\n",
        "                                hidden_size=self.hidden_size)\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size).to(device)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embeddings = self.embedding(input_seq)\n",
        "        hidden_state_seq, new_hidden_state = self.recurrent(embeddings,\n",
        "                                                            hidden_state)\n",
        "        return new_hidden_state\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "encoder = Encoder(len(mr_vocab), HIDDEN_SIZE).to(device)\n",
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfLyI-XeoEZy",
        "outputId": "86ad7b97-58d8-4c61-9348-59ff5973d9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(79, 128)\n",
            "  (recurrent): GRU(128, 128)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "d-nVbrYPh3qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=hidden_size)\n",
        "        self.recurrent = nn.GRU(input_size=hidden_size,\n",
        "                                hidden_size=self.hidden_size)\n",
        "        self.output = nn.Linear(in_features=hidden_size,\n",
        "                                out_features=vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embeddings = self.embedding(input_seq)\n",
        "        _, new_hidden_state = self.recurrent(embeddings, hidden_state)\n",
        "        output = self.output(new_hidden_state)\n",
        "        output = self.softmax(output)\n",
        "        return output, new_hidden_state\n",
        "\n",
        "decoder = Decoder(HIDDEN_SIZE, len(ref_vocab)).to(device)\n",
        "print(decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykfvquQxu7pC",
        "outputId": "a698f428-76fd-451f-b657-5b3563560cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder(\n",
            "  (embedding): Embedding(2750, 128)\n",
            "  (recurrent): GRU(128, 128)\n",
            "  (output): Linear(in_features=128, out_features=2750, bias=True)\n",
            "  (softmax): LogSoftmax(dim=2)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "## Train Function"
      ],
      "metadata": {
        "id": "G_fUrmVSiMnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One training example\n",
        "\n",
        "def train(input_seq, target_seq, target_length, target_vocab, encoder, decoder,\n",
        "          encoder_optim, decoder_optim, loss_fn, teacher_forcing_ratio=0.5):\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "    encoder_hidden_state = encoder.initialize_hidden_state()\n",
        "    encoder_hidden_state = encoder(input_seq, encoder_hidden_state)\n",
        "    decoder_hidden_state = encoder_hidden_state\n",
        "    prev_word = torch.Tensor([target_vocab[START]]).type(torch.long).unsqueeze(dim=-1).to(device)\n",
        "\n",
        "    idx = 1\n",
        "    while idx < target_length:\n",
        "        decoder_output, decoder_hidden_state = decoder(prev_word, decoder_hidden_state)\n",
        "        loss += loss_fn(decoder_output.squeeze(), target_seq[idx].squeeze())\n",
        "        if teacher_forcing_ratio > random.random():\n",
        "            prev_word = target_seq[idx].unsqueeze(dim=-1)\n",
        "        else:\n",
        "            prev_word = decoder_output.argmax(dim=-1).detach()\n",
        "        if prev_word.item() == ref_vocab[END]:\n",
        "            break\n",
        "        idx += 1\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optim.step()\n",
        "    decoder_optim.step()\n",
        "    return loss.item()/target_length"
      ],
      "metadata": {
        "id": "6rLSyNhry6BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimisers and Loss Function"
      ],
      "metadata": {
        "id": "6KzAWTC_iXBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_optim = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_optim = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "otfNzXSDOJ7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Training Loop"
      ],
      "metadata": {
        "id": "T038OflFikJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "epoch_loss = 0\n",
        "while idx < len(train_examples):\n",
        "    input_seq = train_mr_tokens[idx].unsqueeze(dim=-1)\n",
        "    target_seq = train_ref_tokens[idx].unsqueeze(dim=-1)\n",
        "    epoch_loss += train(input_seq, target_seq, 52, ref_vocab, encoder, decoder,\n",
        "                        encoder_optim, decoder_optim, loss_fn, 0.5)\n",
        "    idx += 1\n",
        "    if idx % 200 == 0:\n",
        "        print(f\"{idx} / {len(train_examples)} examples\")"
      ],
      "metadata": {
        "id": "g1Nvw8DUiwKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "fc77746c-bad0-47ae-fdae-2a3ebc923f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 / 42022 examples\n",
            "400 / 42022 examples\n",
            "600 / 42022 examples\n",
            "800 / 42022 examples\n",
            "1000 / 42022 examples\n",
            "1200 / 42022 examples\n",
            "1400 / 42022 examples\n",
            "1600 / 42022 examples\n",
            "1800 / 42022 examples\n",
            "2000 / 42022 examples\n",
            "2200 / 42022 examples\n",
            "2400 / 42022 examples\n",
            "2600 / 42022 examples\n",
            "2800 / 42022 examples\n",
            "3000 / 42022 examples\n",
            "3200 / 42022 examples\n",
            "3400 / 42022 examples\n",
            "3600 / 42022 examples\n",
            "3800 / 42022 examples\n",
            "4000 / 42022 examples\n",
            "4200 / 42022 examples\n",
            "4400 / 42022 examples\n",
            "4600 / 42022 examples\n",
            "4800 / 42022 examples\n",
            "5000 / 42022 examples\n",
            "5200 / 42022 examples\n",
            "5400 / 42022 examples\n",
            "5600 / 42022 examples\n",
            "5800 / 42022 examples\n",
            "6000 / 42022 examples\n",
            "6200 / 42022 examples\n",
            "6400 / 42022 examples\n",
            "6600 / 42022 examples\n",
            "6800 / 42022 examples\n",
            "7000 / 42022 examples\n",
            "7200 / 42022 examples\n",
            "7400 / 42022 examples\n",
            "7600 / 42022 examples\n",
            "7800 / 42022 examples\n",
            "8000 / 42022 examples\n",
            "8200 / 42022 examples\n",
            "8400 / 42022 examples\n",
            "8600 / 42022 examples\n",
            "8800 / 42022 examples\n",
            "9000 / 42022 examples\n",
            "9200 / 42022 examples\n",
            "9400 / 42022 examples\n",
            "9600 / 42022 examples\n",
            "9800 / 42022 examples\n",
            "10000 / 42022 examples\n",
            "10200 / 42022 examples\n",
            "10400 / 42022 examples\n",
            "10600 / 42022 examples\n",
            "10800 / 42022 examples\n",
            "11000 / 42022 examples\n",
            "11200 / 42022 examples\n",
            "11400 / 42022 examples\n",
            "11600 / 42022 examples\n",
            "11800 / 42022 examples\n",
            "12000 / 42022 examples\n",
            "12200 / 42022 examples\n",
            "12400 / 42022 examples\n",
            "12600 / 42022 examples\n",
            "12800 / 42022 examples\n",
            "13000 / 42022 examples\n",
            "13200 / 42022 examples\n",
            "13400 / 42022 examples\n",
            "13600 / 42022 examples\n",
            "13800 / 42022 examples\n",
            "14000 / 42022 examples\n",
            "14200 / 42022 examples\n",
            "14400 / 42022 examples\n",
            "14600 / 42022 examples\n",
            "14800 / 42022 examples\n",
            "15000 / 42022 examples\n",
            "15200 / 42022 examples\n",
            "15400 / 42022 examples\n",
            "15600 / 42022 examples\n",
            "15800 / 42022 examples\n",
            "16000 / 42022 examples\n",
            "16200 / 42022 examples\n",
            "16400 / 42022 examples\n",
            "16600 / 42022 examples\n",
            "16800 / 42022 examples\n",
            "17000 / 42022 examples\n",
            "17200 / 42022 examples\n",
            "17400 / 42022 examples\n",
            "17600 / 42022 examples\n",
            "17800 / 42022 examples\n",
            "18000 / 42022 examples\n",
            "18200 / 42022 examples\n",
            "18400 / 42022 examples\n",
            "18600 / 42022 examples\n",
            "18800 / 42022 examples\n",
            "19000 / 42022 examples\n",
            "19200 / 42022 examples\n",
            "19400 / 42022 examples\n",
            "19600 / 42022 examples\n",
            "19800 / 42022 examples\n",
            "20000 / 42022 examples\n",
            "20200 / 42022 examples\n",
            "20400 / 42022 examples\n",
            "20600 / 42022 examples\n",
            "20800 / 42022 examples\n",
            "21000 / 42022 examples\n",
            "21200 / 42022 examples\n",
            "21400 / 42022 examples\n",
            "21600 / 42022 examples\n",
            "21800 / 42022 examples\n",
            "22000 / 42022 examples\n",
            "22200 / 42022 examples\n",
            "22400 / 42022 examples\n",
            "22600 / 42022 examples\n",
            "22800 / 42022 examples\n",
            "23000 / 42022 examples\n",
            "23200 / 42022 examples\n",
            "23400 / 42022 examples\n",
            "23600 / 42022 examples\n",
            "23800 / 42022 examples\n",
            "24000 / 42022 examples\n",
            "24200 / 42022 examples\n",
            "24400 / 42022 examples\n",
            "24600 / 42022 examples\n",
            "24800 / 42022 examples\n",
            "25000 / 42022 examples\n",
            "25200 / 42022 examples\n",
            "25400 / 42022 examples\n",
            "25600 / 42022 examples\n",
            "25800 / 42022 examples\n",
            "26000 / 42022 examples\n",
            "26200 / 42022 examples\n",
            "26400 / 42022 examples\n",
            "26600 / 42022 examples\n",
            "26800 / 42022 examples\n",
            "27000 / 42022 examples\n",
            "27200 / 42022 examples\n",
            "27400 / 42022 examples\n",
            "27600 / 42022 examples\n",
            "27800 / 42022 examples\n",
            "28000 / 42022 examples\n",
            "28200 / 42022 examples\n",
            "28400 / 42022 examples\n",
            "28600 / 42022 examples\n",
            "28800 / 42022 examples\n",
            "29000 / 42022 examples\n",
            "29200 / 42022 examples\n",
            "29400 / 42022 examples\n",
            "29600 / 42022 examples\n",
            "29800 / 42022 examples\n",
            "30000 / 42022 examples\n",
            "30200 / 42022 examples\n",
            "30400 / 42022 examples\n",
            "30600 / 42022 examples\n",
            "30800 / 42022 examples\n",
            "31000 / 42022 examples\n",
            "31200 / 42022 examples\n",
            "31400 / 42022 examples\n",
            "31600 / 42022 examples\n",
            "31800 / 42022 examples\n",
            "32000 / 42022 examples\n",
            "32200 / 42022 examples\n",
            "32400 / 42022 examples\n",
            "32600 / 42022 examples\n",
            "32800 / 42022 examples\n",
            "33000 / 42022 examples\n",
            "33200 / 42022 examples\n",
            "33400 / 42022 examples\n",
            "33600 / 42022 examples\n",
            "33800 / 42022 examples\n",
            "34000 / 42022 examples\n",
            "34200 / 42022 examples\n",
            "34400 / 42022 examples\n",
            "34600 / 42022 examples\n",
            "34800 / 42022 examples\n",
            "35000 / 42022 examples\n",
            "35200 / 42022 examples\n",
            "35400 / 42022 examples\n",
            "35600 / 42022 examples\n",
            "35800 / 42022 examples\n",
            "36000 / 42022 examples\n",
            "36200 / 42022 examples\n",
            "36400 / 42022 examples\n",
            "36600 / 42022 examples\n",
            "36800 / 42022 examples\n",
            "37000 / 42022 examples\n",
            "37200 / 42022 examples\n",
            "37400 / 42022 examples\n",
            "37600 / 42022 examples\n",
            "37800 / 42022 examples\n",
            "38000 / 42022 examples\n",
            "38200 / 42022 examples\n",
            "38400 / 42022 examples\n",
            "38600 / 42022 examples\n",
            "38800 / 42022 examples\n",
            "39000 / 42022 examples\n",
            "39200 / 42022 examples\n",
            "39400 / 42022 examples\n",
            "39600 / 42022 examples\n",
            "39800 / 42022 examples\n",
            "40000 / 42022 examples\n",
            "40200 / 42022 examples\n",
            "40400 / 42022 examples\n",
            "40600 / 42022 examples\n",
            "40800 / 42022 examples\n",
            "41000 / 42022 examples\n",
            "41200 / 42022 examples\n",
            "41400 / 42022 examples\n",
            "41600 / 42022 examples\n",
            "41800 / 42022 examples\n",
            "42000 / 42022 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving our model"
      ],
      "metadata": {
        "id": "wpXLkmaEinx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), 'encoder.pth')\n",
        "torch.save(decoder.state_dict(), 'decoder.pth')"
      ],
      "metadata": {
        "id": "j2DovaGlsa1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('encoder.pth')\n",
        "files.download('decoder.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nbg1wH9-84Mo",
        "outputId": "cc3308c2-b224-430f-ec81-60f86a82377e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9b76987b-80f3-4b60-b071-d2a421fb5daf\", \"encoder.pth\", 438552)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_620640a9-049d-44ed-b92a-cc684985b160\", \"decoder.pth\", 3225488)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "## Loading the test examples"
      ],
      "metadata": {
        "id": "20T7Eovfiq2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples = get_examples('test.txt')\n",
        "test_examples, test_ref_tokens, test_mr_tokens, test_lengths = extract_mr_ref(test_examples)\n",
        "test_ref_tokens = [torch.Tensor([ref_vocab[token] for token in sentence]).type(torch.long).to(device) for sentence in test_ref_tokens]\n",
        "test_mr_tokens = [torch.Tensor([mr_vocab[token] for token in sentence]).type(torch.long).to(device) for sentence in test_mr_tokens]"
      ],
      "metadata": {
        "id": "VKx6Qse4Ifk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation Function"
      ],
      "metadata": {
        "id": "M7qkioFIiw2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input_seq, target_length, target_vocab, encoder, decoder):\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden_state = encoder.initialize_hidden_state()\n",
        "        encoder_hidden_state = encoder(input_seq, encoder_hidden_state)\n",
        "        decoder_hidden_state = encoder_hidden_state\n",
        "        prev_word = torch.Tensor([target_vocab[START]]).type(torch.long).unsqueeze(dim=-1).to(device)\n",
        "\n",
        "        idx = 1\n",
        "        while idx < target_length:\n",
        "            decoder_output, decoder_hidden_state = decoder(prev_word, decoder_hidden_state)\n",
        "            prev_word = decoder_output.argmax(dim=-1)\n",
        "            print(f\"{ref_vocab.lookup_token(prev_word.item())}\", end=\" \")\n",
        "            idx += 1"
      ],
      "metadata": {
        "id": "nBjhUZxmI_Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(test_mr_tokens[1].unsqueeze(dim=-1), 52, ref_vocab, encoder, decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UarD9nFEJhsT",
        "outputId": "6804e845-ac54-4719-dc9e-95ae1e249269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alimentum is a family-friendly restaurant in the city centre . it is not family-friendly . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    idx = random.randint(0, len(test_examples)-1)\n",
        "    input_seq = test_mr_tokens[idx].unsqueeze(dim=-1)\n",
        "    print(test_examples[idx])\n",
        "    generate(input_seq, 50, ref_vocab, encoder, decoder)\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq25Hi33KLUQ",
        "outputId": "0f85dd88-44c9-4e76-cfff-f43d66246f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('name[Cotto], eatType[coffee shop], food[English], priceRange[Â£20-25], customer rating[high], area[riverside], near[The Portland Arms]', 'Located on the river near The Portland Arms, The Cotto offers a classy place to grab a bite with its five star rating.')\n",
            "the portland arms , the portland arms , the wrestlers is a high customer rating and a price range of â£20-25 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Aromi], eatType[coffee shop], food[Chinese], customer rating[average], area[riverside], familyFriendly[yes]', 'Aromi is a family friendly Chinese food coffee shop in the riverside area.')\n",
            "aromi is a coffee shop that serves indian food in the riverside area . it is a friendly . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Wrestlers], customer rating[3 out of 5], familyFriendly[yes]', 'There is a child friendly place named The Wrestlers with a 3 out of 5 rating.')\n",
            "the wrestlers is a kid friendly restaurant with a customer rating of 3 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Cocum], eatType[coffee shop], food[Chinese], priceRange[Â£20-25], customer rating[high], familyFriendly[yes]', 'Cocum is a coffee shop providing Chinese food in the Â£20-25 price range. Its customer rating is high.')\n",
            "cocum is a coffee shop that serves indian food and has a price range of â£20-25 . its customer rating is 3 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Browns Cambridge], eatType[coffee shop], food[Chinese], customer rating[3 out of 5], area[riverside], familyFriendly[yes], near[Crowne Plaza Hotel]', 'Near the Crowne Plaza Hotel at the riverside there is a coffee ship called Browns Cambridge which serves Chinese food. It has a customer rating of 3 out of 5 and is kid friendly.')\n",
            "browns cambridge is a kid friendly coffee shop located near the crowne plaza hotel in the riverside area . it has a customer rating of 3 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Taste of Cambridge], eatType[coffee shop], food[Chinese], area[riverside], familyFriendly[yes], near[Crowne Plaza Hotel]', 'Taste of Cambridge is a coffee shop that is child friendly and serves Chinese food. It is located in riverside near the Crowne Plaza Hotel.')\n",
            "the is a coffee shop called the riverside that serves indian food and is children friendly . it is located near the crowne plaza hotel . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Taste of Cambridge], eatType[coffee shop], food[English], area[riverside], familyFriendly[yes], near[Crowne Plaza Hotel]', 'If you looking for children-friendly coffee shop with English food go to Taste of Cambridge near Crowne Plaza Hotel.')\n",
            "the is a coffee shop called the crowne plaza hotel in the riverside area called the dumpling tree . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Wildwood], eatType[coffee shop], food[English], priceRange[moderate], customer rating[1 out of 5], near[Ranch]', 'Wildwood is a mid cost coffee shop. It is poorly rated and is located by the Ranch')\n",
            "the wildwood is a coffee shop serving english food with a price range of ranch and has a customer rating of 1 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Punter], eatType[coffee shop], food[English], priceRange[less than Â£20], customer rating[low], familyFriendly[yes], near[CafÃ© Sicilia]', \"The Punter is a cheap family friendly coffee shop serving English food. It is located near CafÃ© Sicilia however it's rated low.\")\n",
            "the punter is a family friendly coffee shop near cafã© sicilia that serves low food . has a low customer rating . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Wrestlers], customer rating[high], familyFriendly[yes], near[The Sorrento]', 'Located near The Sorrento is a kid-friendly place named The Wrestlers. It has a high customer rating.')\n",
            "the wrestlers is a high customer rating and is kid friendly . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Phoenix], customer rating[average], area[city centre]', 'There is a place called The Phoenix near the city centre with an average customer rating.')\n",
            "the phoenix is a in the city centre . it has a customer rating of 3 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Eagle], eatType[coffee shop], food[English], priceRange[Â£20-25], customer rating[high], area[city centre], familyFriendly[yes], near[Burger King]', 'The Eagle is an average English coffee shop located in city centre near Burger King. It is highly rated by customers and is kids friendly.')\n",
            "the eagle is a coffee shop located near the city centre near burger king . it is a friendly and has a high customer rating and a price range of â£20-25 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Eagle], eatType[coffee shop], food[English], priceRange[moderate], customer rating[3 out of 5], area[riverside], familyFriendly[no], near[Burger King]', 'The Eagle is an English coffee shop near Burger King in riverside with has a moderate price range, is not kids friendly, and has a 3 out of 5 customer rating.')\n",
            "the eagle is a coffee shop serving moderately priced food . it is located in the riverside area near burger king . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Eagle], customer rating[low], area[riverside], familyFriendly[yes], near[CafÃ© Brazil]', 'A Family friendly restaurant near CafÃ© Brazil by the riverside is The Eagle but it ranks low')\n",
            "the cafã© rouge is the riverside area is a family friendly place called the eagle . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Punter], eatType[coffee shop], food[Chinese], priceRange[high], customer rating[1 out of 5], familyFriendly[no], near[CafÃ© Sicilia]', 'The Punter is an adult coffee shop that serves Chinese food.  It is expensive, rated low by its customers, and is located near CafÃ© Sicilia.')\n",
            "the punter is a coffee shop providing indian food in the high price range . it is located near cafã© sicilia . its customer rating is 1 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Cotto], eatType[coffee shop], food[Chinese], priceRange[moderate], customer rating[3 out of 5], area[riverside], near[The Portland Arms]', \"in riverside by The Portland Arms there's a moderately priced coffee shop called Cotto with a rating of 3 out of 5 that serves Chinese food.\")\n",
            "cotto is a coffee shop shop located near the portland arms in the riverside area . it has a moderate price range . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Bibimbap House], area[city centre], near[CafÃ© Sicilia]', 'A restaurant called Bibimbap House is located near CafÃ© Sicilia in the city centre area.')\n",
            "bibimbap house is a cheap restaurant located near cafã© sicilia . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[Wildwood], eatType[coffee shop], food[English], priceRange[moderate], customer rating[3 out of 5], near[Ranch]', 'Wildwood is a mid priced coffee shop located next to Ranch.')\n",
            "wildwood is a coffee shop serving moderately priced food . it is located near ranch . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Wrestlers], customer rating[5 out of 5], familyFriendly[yes]', 'The Wrestlers has a rating of 5 out 5. It is also family friendly.')\n",
            "the wrestlers is a family friendly restaurant with a customer rating of 5 out of 5 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n",
            "('name[The Wrestlers], eatType[coffee shop], food[Chinese], priceRange[less than Â£20], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]', 'If you want a low-priced coffee shop on the riverside, then try The Wrestlers. This family-friendly coffee shop also serves Chinese food and it is located near Raja Indian Cuisine')\n",
            "the wrestlers is a coffee shop that indian food in the less than â£20 price range . it is located in the riverside near raja indian cuisine . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amqeB2FRKsxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}